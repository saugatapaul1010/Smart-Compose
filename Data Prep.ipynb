{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing all the things we'll need.\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, CuDNNLSTM, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "tf.__version__\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20010101</td>\n",
       "      <td>sports.wwe</td>\n",
       "      <td>win over cena satisfying but defeating underta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010102</td>\n",
       "      <td>bollywood</td>\n",
       "      <td>Raju Chacha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Status quo will not be disturbed at Ayodhya; s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Fissures in Hurriyat over Pak visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>America's unwanted heading for India?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_category  \\\n",
       "0      20010101        sports.wwe   \n",
       "1      20010102         bollywood   \n",
       "2      20010102           unknown   \n",
       "3      20010102           unknown   \n",
       "4      20010102           unknown   \n",
       "\n",
       "                                       headline_text  \n",
       "0  win over cena satisfying but defeating underta...  \n",
       "1                                        Raju Chacha  \n",
       "2  Status quo will not be disturbed at Ayodhya; s...  \n",
       "3                Fissures in Hurriyat over Pak visit  \n",
       "4              America's unwanted heading for India?  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_news.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sent):\n",
    "    string=re.sub(r'\\([^)]*\\)', ' ', sent)\n",
    "    string=re.sub(r'\\[[^)]*\\]', ' ', string)\n",
    "    string=string.replace(\"\\n\",\" \")\n",
    "    string=string.strip()\n",
    "    string=re.sub('[^.a-zA-Z]',' ',string)\n",
    "    string=string.lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>processed_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20010101</td>\n",
       "      <td>sports.wwe</td>\n",
       "      <td>win over cena satisfying but defeating underta...</td>\n",
       "      <td>win over cena satisfying but defeating underta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010102</td>\n",
       "      <td>bollywood</td>\n",
       "      <td>Raju Chacha</td>\n",
       "      <td>raju chacha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Status quo will not be disturbed at Ayodhya; s...</td>\n",
       "      <td>status quo will not be disturbed at ayodhya  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Fissures in Hurriyat over Pak visit</td>\n",
       "      <td>fissures in hurriyat over pak visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>America's unwanted heading for India?</td>\n",
       "      <td>america s unwanted heading for india</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_category  \\\n",
       "0      20010101        sports.wwe   \n",
       "1      20010102         bollywood   \n",
       "2      20010102           unknown   \n",
       "3      20010102           unknown   \n",
       "4      20010102           unknown   \n",
       "\n",
       "                                       headline_text  \\\n",
       "0  win over cena satisfying but defeating underta...   \n",
       "1                                        Raju Chacha   \n",
       "2  Status quo will not be disturbed at Ayodhya; s...   \n",
       "3                Fissures in Hurriyat over Pak visit   \n",
       "4              America's unwanted heading for India?   \n",
       "\n",
       "                                      processed_news  \n",
       "0  win over cena satisfying but defeating underta...  \n",
       "1                                        raju chacha  \n",
       "2  status quo will not be disturbed at ayodhya  s...  \n",
       "3                fissures in hurriyat over pak visit  \n",
       "4              america s unwanted heading for india   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_news']=df['headline_text'].apply(process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list=[]\n",
    "for i in df['processed_news'].values:\n",
    "    length=len(i.split())\n",
    "    if(length>12):\n",
    "        string_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94708"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "file = open(\"demo_text.txt\",\"r\") \n",
    "string=file.read()\n",
    "\n",
    "string=re.sub(r'\\([^)]*\\)', '', string)\n",
    "string=re.sub(r'\\[[^)]*\\]', '', string)\n",
    "string=string.replace(\"\\n\",\"\")\n",
    "string=string.strip()\n",
    "string=re.sub('[^.a-zA-Z]',' ',string)\n",
    "string=string.lower()\n",
    "string_list=string.split(\".\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26465"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=\" \".join(string_list)\n",
    "corpus=corpus+\" \"+\"<start>\"+\" \"+\"<end>\"\n",
    "corpus_words=corpus.split()\n",
    "corpus_set=list(set(sorted(corpus_words)))\n",
    "len(corpus_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index= {word: index for (index, word) in enumerate(corpus_set)}\n",
    "index_to_word = {index: word for (word, index) in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    X_data=[]\n",
    "    Y_data=[]\n",
    "\n",
    "    for sent in string_list:\n",
    "        for i in range(len(sent.split())):\n",
    "            X=['<start> ']+ sent.split()[:i+1] + [' <end>']\n",
    "            X=\" \".join(X)\n",
    "            Y=['<start> ']+ sent.split()[i+1:] + [' <end>']\n",
    "            Y=\" \".join(Y)\n",
    "            X_data.append(X)\n",
    "            Y_data.append(Y)\n",
    "\n",
    "    data=pd.DataFrame()\n",
    "    data[\"input\"]=X_data\n",
    "    data[\"output\"]=Y_data\n",
    "    \n",
    "    return data\n",
    "\n",
    "def encode_string(string):\n",
    "    encoded_list=[]\n",
    "    raw_words=string.split()\n",
    "    for word in raw_words:\n",
    "        idx=word_to_index[word]\n",
    "        encoded_list.append(idx)\n",
    "        \n",
    "    return encoded_list\n",
    "\n",
    "def pad_sequences(input_list, max_len):\n",
    "    pads=max_len-len(input_list)\n",
    "    zero_seq=[0]*pads\n",
    "    input_list.extend(zero_seq)\n",
    "    return input_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt;  dancing  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  is a pleasure but give us hot food an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt;  dancing is  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  a pleasure but give us hot food and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt;  dancing is a  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  pleasure but give us hot food and wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt;  dancing is a pleasure  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  but give us hot food and water say ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt;  dancing is a pleasure but  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  give us hot food and water say artist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input  \\\n",
       "0                    <start>  dancing  <end>   \n",
       "1                 <start>  dancing is  <end>   \n",
       "2               <start>  dancing is a  <end>   \n",
       "3      <start>  dancing is a pleasure  <end>   \n",
       "4  <start>  dancing is a pleasure but  <end>   \n",
       "\n",
       "                                              output  \n",
       "0  <start>  is a pleasure but give us hot food an...  \n",
       "1  <start>  a pleasure but give us hot food and w...  \n",
       "2  <start>  pleasure but give us hot food and wat...  \n",
       "3  <start>  but give us hot food and water say ar...  \n",
       "4  <start>  give us hot food and water say artist...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=generate_dataset()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['encoded_input']=data['input'].apply(encode_string)\n",
    "data['encoded_output']=data['output'].apply(encode_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>encoded_input</th>\n",
       "      <th>encoded_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt;  dancing  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  is a pleasure but give us hot food an...</td>\n",
       "      <td>[22342, 5702, 23161]</td>\n",
       "      <td>[22342, 13389, 16238, 21711, 15314, 14516, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt;  dancing is  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  a pleasure but give us hot food and w...</td>\n",
       "      <td>[22342, 5702, 13389, 23161]</td>\n",
       "      <td>[22342, 16238, 21711, 15314, 14516, 19903, 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt;  dancing is a  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  pleasure but give us hot food and wat...</td>\n",
       "      <td>[22342, 5702, 13389, 16238, 23161]</td>\n",
       "      <td>[22342, 21711, 15314, 14516, 19903, 12174, 151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt;  dancing is a pleasure  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  but give us hot food and water say ar...</td>\n",
       "      <td>[22342, 5702, 13389, 16238, 21711, 23161]</td>\n",
       "      <td>[22342, 15314, 14516, 19903, 12174, 15180, 523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt;  dancing is a pleasure but  &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;  give us hot food and water say artist...</td>\n",
       "      <td>[22342, 5702, 13389, 16238, 21711, 15314, 23161]</td>\n",
       "      <td>[22342, 14516, 19903, 12174, 15180, 5239, 2508...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input  \\\n",
       "0                    <start>  dancing  <end>   \n",
       "1                 <start>  dancing is  <end>   \n",
       "2               <start>  dancing is a  <end>   \n",
       "3      <start>  dancing is a pleasure  <end>   \n",
       "4  <start>  dancing is a pleasure but  <end>   \n",
       "\n",
       "                                              output  \\\n",
       "0  <start>  is a pleasure but give us hot food an...   \n",
       "1  <start>  a pleasure but give us hot food and w...   \n",
       "2  <start>  pleasure but give us hot food and wat...   \n",
       "3  <start>  but give us hot food and water say ar...   \n",
       "4  <start>  give us hot food and water say artist...   \n",
       "\n",
       "                                      encoded_input  \\\n",
       "0                              [22342, 5702, 23161]   \n",
       "1                       [22342, 5702, 13389, 23161]   \n",
       "2                [22342, 5702, 13389, 16238, 23161]   \n",
       "3         [22342, 5702, 13389, 16238, 21711, 23161]   \n",
       "4  [22342, 5702, 13389, 16238, 21711, 15314, 23161]   \n",
       "\n",
       "                                      encoded_output  \n",
       "0  [22342, 13389, 16238, 21711, 15314, 14516, 199...  \n",
       "1  [22342, 16238, 21711, 15314, 14516, 19903, 121...  \n",
       "2  [22342, 21711, 15314, 14516, 19903, 12174, 151...  \n",
       "3  [22342, 15314, 14516, 19903, 12174, 15180, 523...  \n",
       "4  [22342, 14516, 19903, 12174, 15180, 5239, 2508...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_input=max([len(i) for i in data.encoded_input.values])\n",
    "max_length_output=max([len(i) for i in data.encoded_output.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=[i for i in data.encoded_input.values]\n",
    "output_data=[i for i in data.encoded_output.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['padded_input']=data['encoded_input'].apply(lambda x: pad_sequences(x, max_length_input))\n",
    "data['padded_output']=data['encoded_output'].apply(lambda x: pad_sequences(x, max_length_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "padded_input=np.array([i for i in data.padded_input.values])\n",
    "padded_output=np.array([i for i in data.padded_output.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = [[padded_output[n][i+1] for i in range(len(padded_output[n])-1)] for n in range(len(padded_output))]\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=max_length_output, padding=\"post\")\n",
    "target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a96c518336ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mteacher_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "p = np.random.permutation(len(input_data))\n",
    "input_data = input_data[p]\n",
    "teacher_data = teacher_data[p]\n",
    "target_data = target_data[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "BUFFER_SIZE = len(input_data)\n",
    "BATCH_SIZE = 128\n",
    "embedding_dim = 300\n",
    "units = 128\n",
    "vocab_in_size = len(word_to_index.keys())\n",
    "vocab_out_size = len(word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26465"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 27)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 27, 300)      7939500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 27, 256), (N 439296      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    7939500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][3]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  570368      embedding_1[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 128)    32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 26465)  3413985     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,335,545\n",
      "Trainable params: 20,335,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Encoder layers first.\n",
    "encoder_inputs = Input(shape=(max_length_input,))\n",
    "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "\n",
    "# Use this if you dont need Bidirectional LSTM\n",
    "# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "\n",
    "encoder_lstm = Bidirectional(CuDNNLSTM(units=units, return_sequences=True, return_state=True))\n",
    "encoder_outputs, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "state_h = Concatenate()([fstate_h,bstate_h])\n",
    "state_c = Concatenate()([bstate_h,bstate_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Now create the Decoder layers.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "decoder_lstm = LSTM(units=units*2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([\n",
    "    decoder_outputs, attn_out])\n",
    "\n",
    "\n",
    "# Two dense layers added to this model to improve inference capabilities.\n",
    "decoder_d1 = Dense(units, activation=\"relu\")\n",
    "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_outputs))))\n",
    "\n",
    "\n",
    "# Finally, create a training model which combines the encoder and the decoder.\n",
    "# Note that this model has three inputs:\n",
    "model = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
    "\n",
    "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
    "# Adam is used because it's, well, the best.\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282066 samples, validate on 70517 samples\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      " 12032/282066 [>.............................] - ETA: 2:46:24 - loss: 3.5068 - sparse_categorical_accuracy: 0.7015"
     ]
    }
   ],
   "source": [
    "# Note, we use 20% of our data for validation.\n",
    "epochs = 10\n",
    "history = model.fit([input_data, padded_output], target_data,\n",
    "                 batch_size= BATCH_SIZE,\n",
    "                 epochs=epochs,\n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of the training.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"Training loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder model from the tensors we previously declared.\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
    "\n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(units*2,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units*2,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
    "    decoder_emb(inf_decoder_inputs), \n",
    "    initial_state=[state_input_h, state_input_c])\n",
    "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the given sentence (just a string) into a vector of word IDs\n",
    "# Output is 1-D: [timesteps/words]\n",
    "\n",
    "def sentence_to_vector(sentence):\n",
    "\n",
    "    pre = sentence\n",
    "    vec = np.zeros(max_length_input)\n",
    "    sentence_list = [word_to_index[s] for s in pre.split(' ')]\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vec[i] = w\n",
    "    return vec\n",
    "\n",
    "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
    "def translate(input_sentence, infenc_model, infmodel):\n",
    "    sv = sentence_to_vector(input_sentence)\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = word_to_index[\"<start>\"]\n",
    "    stop_vec = word_to_index[\"<end>\"]\n",
    "    \n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "\n",
    "    while cur_word != \"<end>\" and i < (max_length_output-1):\n",
    "        i += 1\n",
    "        if cur_word != \"<start>\":\n",
    "            output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        cur_word = index_to_word[np.argmax(nvec[0,0])]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input seq</th>\n",
       "      <th>Pred. Seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jose is the</td>\n",
       "      <td>the agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he is</td>\n",
       "      <td>the agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificial</td>\n",
       "      <td>the agent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Input seq   Pred. Seq\n",
       "0  jose is the   the agent\n",
       "1  he is         the agent\n",
       "2  artificial    the agent"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n",
    "\n",
    "\n",
    "test = [\n",
    "    'jose is the',\n",
    "    'he is',\n",
    "    'artificial'\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "output = []  \n",
    "for t in test:  \n",
    "  output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate(t.lower(), encoder_model, inf_model)})\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(output) \n",
    "results_df.head(len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
